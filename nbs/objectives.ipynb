{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp opt.objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import operator\n",
    "import torch\n",
    "from abc import ABC\n",
    "import fastcore.all as fc\n",
    "from torch import nn\n",
    "from translucid.opt.hooks import *\n",
    "from translucid.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "class Objective(ABC):\n",
    "    def __init__(self, loss_fn, repr=None):\n",
    "        self.loss_fn, self.repr = loss_fn, repr\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.repr or self.__class__.__name__\n",
    "\n",
    "    @staticmethod\n",
    "    def _store_hook(hook, module, input, output):\n",
    "        hook.value = output\n",
    "\n",
    "    def __call__(self, model: nn.Module, x):\n",
    "        with NamedHooks(model.named_modules(), fn=self._store_hook) as hooks: model(x)\n",
    "        return self.eval(hooks)\n",
    "\n",
    "    def eval(self, hooks):\n",
    "        return self.loss_fn(hooks)\n",
    "\n",
    "    def _op(self, other, op):\n",
    "        if isinstance(other, (int, float)):\n",
    "            fn = lambda hooks: op(self.eval(hooks), other)\n",
    "        elif isinstance(other, Objective):\n",
    "            fn = lambda hooks: op(self.eval(hooks), other.eval(hooks))\n",
    "        return Objective(fn, repr=f'{self} {op2str(op)} {other}')\n",
    "\n",
    "    def __add__(self, other): return self._op(other, operator.add)\n",
    "    def __radd__(self, other): return self + other\n",
    "    def __mul__(self, other): return self._op(other, operator.mul)\n",
    "    def __rmul__(self, other): return self * other\n",
    "    def __sub__(self, other): return self._op(other, operator.sub)\n",
    "    def __rsub__(self, other): return -self + other\n",
    "    def __pow__(self, other): return self._op(other, operator.pow)\n",
    "    def __rpow__(self, other): return Objective(lambda hooks: other ** self.eval(hooks))\n",
    "    def __truediv__(self, other): return self._op(other, operator.truediv)\n",
    "    def __rtruediv__(self, other): return self ** -1 * other\n",
    "    def __neg__(self): return self * -1\n",
    "\n",
    "\n",
    "class ModuleObjective(Objective):\n",
    "    def __init__(self, module_name, loss_fn, repr):\n",
    "        super().__init__(loss_fn, repr)\n",
    "        self.module_name = module_name\n",
    "\n",
    "    def eval(self, hooks):\n",
    "        return self.loss_fn(hooks[self.module_name].value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def neuron(layer, n_channel, x=None, y=None, batch=None):\n",
    "    \"\"\"\n",
    "    Visualize a specific neuron within a given channel. \n",
    "    If no (x, y) coordinates are provided, the neuron at the center is chosen. \n",
    "    For layers with even width and height, the bottom-right neuron of the central 2x2 region is selected by default.\n",
    "    \"\"\"\n",
    "    def loss_fn(out):\n",
    "        assert len(out.shape) == 4\n",
    "        x_pos, y_pos = x or out.shape[-1] // 2, y or out.shape[-2] // 2\n",
    "        return out[fc.ifnone(batch, slice(batch)), n_channel, y_pos, x_pos].mean()\n",
    "    r = to_repr(f\"n{n_channel}_neuron\", layer=layer, x=x, y=y, batch=batch)\n",
    "    return ModuleObjective(layer, loss_fn, repr=r)\n",
    "\n",
    "\n",
    "def channel(layer, n_channel, batch=None):\n",
    "    \"\"\"Visualize an entire channel of a layer.\"\"\"\n",
    "    def loss_fn(out):\n",
    "        return out[fc.ifnone(batch, slice(batch)), n_channel].mean()\n",
    "    r = to_repr(f\"n{n_channel}_channel\", layer=layer, batch=batch)\n",
    "    return ModuleObjective(layer, loss_fn, repr=r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n5_neuron(layer='1', batch=0) + n1_channel(layer='1', batch=1) + n2_channel(layer='2', batch=2)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3, 1, 28, 28)\n",
    "obj = neuron('1', 5, batch=0) + channel('1', 1, batch=1) + channel('2', 2, batch=2)\n",
    "obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0591, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 32, 3),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(32, 64, 3))\n",
    "\n",
    "obj(model, x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
